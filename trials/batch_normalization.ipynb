{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T03:13:21.313264Z",
     "start_time": "2025-04-21T03:13:21.305119Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not torch.is_grad_enabled():  # 如果处于推理模式（非训练模式）\n",
    "        # 使用历史的移动平均均值和方差进行归一化（测试时）\n",
    "        # moving_mean, moving_var是全局的均值和方差\n",
    "        # sqrt是平方根运算\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        # X 维度必须是2或4（全连接层输出 or 卷积层输出）\n",
    "        assert len(X.shape) in (2, 4)\n",
    "\n",
    "        if len(X.shape) == 2:  # 全连接层\n",
    "            # 计算每一列（特征）的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:  # 卷积层，shape = (batch_size, channels, height, width)\n",
    "            # 在 batch、height 和 width 维度上做均值（每个通道一个均值）\n",
    "            # 输出形状1*n*1*1\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "        # 归一化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "        # 更新移动平均（用于测试阶段）\n",
    "        moving_mean[:] = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var[:] = momentum * moving_var + (1.0 - momentum) * var\n",
    "\n",
    "    # 缩放和平移（通过可学习参数 gamma 和 beta 恢复表达能力）\n",
    "    return gamma * X_hat + beta, moving_mean.data, moving_var.data\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T03:13:21.322151Z",
     "start_time": "2025-04-21T03:13:21.313264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        # 根据输入维度确定归一化形状\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "\n",
    "        # 可学习的参数 gamma（缩放）和 beta（偏移）\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "\n",
    "        # 非学习参数：用于推理阶段的均值和方差\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果当前设备与参数设备不同，则拷贝到相同设备上（GPU）\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "\n",
    "        # 使用辅助函数 batch_norm 进行归一化\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta,\n",
    "            self.moving_mean, self.moving_var,\n",
    "            eps=1e-5, momentum=0.9\n",
    "        )\n",
    "        return Y\n"
   ],
   "id": "bb8973ad0976b167",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T03:14:51.111230Z",
     "start_time": "2025-04-21T03:13:21.327813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 运用batch normalization到LeNet模型\n",
    "net = nn.Sequential(\n",
    "    # 第一层卷积：输入通道 1，输出通道 6，卷积核大小 5x5\n",
    "    nn.Conv2d(1, 6, kernel_size=5),\n",
    "    BatchNorm(6, num_dims=4),              # 对卷积输出做批归一化（4维输入）\n",
    "\n",
    "    nn.Sigmoid(),                          # 激活函数\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # 最大池化，窗口2x2，步幅2\n",
    "\n",
    "    # 第二层卷积：输入通道 6，输出通道 16，卷积核 5x5\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    BatchNorm(16, num_dims=4),             # 对第二层卷积输出做批归一化\n",
    "\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # 再做一次池化\n",
    "\n",
    "    nn.Flatten(),                          # 展平为全连接层输入\n",
    "\n",
    "    # 全连接层：输入尺寸 16×4×4 = 256，输出 120\n",
    "    nn.Linear(16 * 4 * 4, 120),\n",
    "    BatchNorm(120, num_dims=2),            # 对全连接层输出归一化\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(120, 84),                    # 第二个全连接层\n",
    "    BatchNorm(84, num_dims=2),             # 再归一化一次\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(84, 10)                      # 输出层，10类\n",
    ")\n",
    "\n"
   ],
   "id": "9eb54eca5ccfa35a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "loss 0.243, train acc 0.911, test acc 0.885\n",
      "24795.7 examples/sec on cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ml_utils import *\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr, num_epochs = 0.9, 10\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ],
   "id": "581c2395429803e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T03:20:08.197699Z",
     "start_time": "2025-04-21T03:20:08.190189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 简洁实现\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 第1层卷积：输入通道数=1（灰度图），输出通道数=6，卷积核大小5x5\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "\n",
    "    # 批归一化：用于稳定训练过程，加快收敛\n",
    "    nn.BatchNorm2d(num_features=6),\n",
    "\n",
    "    # 激活函数：Sigmoid，用于引入非线性\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 平均池化：池化窗口为2x2，步长为2，用于降采样\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # 第2层卷积：输入通道6，输出通道16，卷积核5x5\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "\n",
    "    # 对第2层卷积后的输出做归一化\n",
    "    nn.BatchNorm2d(num_features=16),\n",
    "\n",
    "    # 再次使用激活函数\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 再次进行平均池化，进一步压缩空间尺寸\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # 展平：将 [batch_size, 16, 4, 4] 展平为 [batch_size, 256]\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # 第1个全连接层：输入256维，输出120维\n",
    "    nn.Linear(in_features=256, out_features=120),\n",
    "\n",
    "    # 对全连接层输出做归一化\n",
    "    nn.BatchNorm1d(num_features=120),\n",
    "\n",
    "    # 激活函数\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 第2个全连接层：120 -> 84\n",
    "    nn.Linear(in_features=120, out_features=84),\n",
    "\n",
    "    # 批归一化\n",
    "    nn.BatchNorm1d(num_features=84),\n",
    "\n",
    "    # 激活函数\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 输出层：最后输出10类（对应数字 0~9）\n",
    "    nn.Linear(in_features=84, out_features=10)\n",
    ")\n"
   ],
   "id": "693ea20f51d832d4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T03:21:30.917629Z",
     "start_time": "2025-04-21T03:20:10.264040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ml_utils import *\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr, num_epochs = 0.9, 10\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ],
   "id": "f4fa262fcda2b43d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "loss 0.264, train acc 0.902, test acc 0.826\n",
      "36605.8 examples/sec on cuda:0\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
