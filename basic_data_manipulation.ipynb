{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.739910Z",
     "start_time": "2025-02-20T08:17:33.711222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 从0到12（开区间）结束的所有整数组成的向量\n",
    "x = torch.arange(12)\n",
    "x"
   ],
   "id": "5444474184cbaa2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.751407Z",
     "start_time": "2025-02-20T08:17:33.746914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 向量的形状\n",
    "x.shape"
   ],
   "id": "66ec6533a6280cc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.770101Z",
     "start_time": "2025-02-20T08:17:33.766585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 向量中元素的个数\n",
    "x.numel()"
   ],
   "id": "f78b1ae360eb8936",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.808225Z",
     "start_time": "2025-02-20T08:17:33.787060Z"
    }
   },
   "cell_type": "code",
   "source": "torch.zeros(2,3,4)",
   "id": "4c231ae67fc652a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.834999Z",
     "start_time": "2025-02-20T08:17:33.813725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1.0,2,3,4,5])\n",
    "y = torch.tensor([2,2,2,2,2])\n",
    "# 按元素的运算\n",
    "# 倒数第二个是x的y次冥\n",
    "# 倒数第一个是每个元素的e指数\n",
    "x+y, x-y, x*y, x/y, x**y, torch.exp(x)"
   ],
   "id": "4f627b705f01fb27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4., 5., 6., 7.]),\n",
       " tensor([-1.,  0.,  1.,  2.,  3.]),\n",
       " tensor([ 2.,  4.,  6.,  8., 10.]),\n",
       " tensor([0.5000, 1.0000, 1.5000, 2.0000, 2.5000]),\n",
       " tensor([ 1.,  4.,  9., 16., 25.]),\n",
       " tensor([  2.7183,   7.3891,  20.0855,  54.5981, 148.4132]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.847033Z",
     "start_time": "2025-02-20T08:17:33.840818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "y = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "# 其中这里的dim0是列，dim1是行\n",
    "torch.cat((x,y), dim=0), torch.cat((x,y), dim=1)"
   ],
   "id": "220f067d1a485265",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.,  8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:33.861688Z",
     "start_time": "2025-02-20T08:17:33.856306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 广播机制，当张量形状不一样的时候会自动扩充\n",
    "x = torch.arange(3, dtype=torch.float32).reshape((1,3))\n",
    "y = torch.arange(2, dtype=torch.float32).reshape((2,1))\n",
    "# 它会把原来的张量给复制成一个最小公倍数的形状，再进行相加\n",
    "x, y, x+y"
   ],
   "id": "e7b3d62f4dacc6b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.]]),\n",
       " tensor([[0.],\n",
       "         [1.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [1., 2., 3.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:20:33.059659Z",
     "start_time": "2025-02-20T08:20:33.054574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(3, dtype=torch.float32).reshape((1,1,3))\n",
    "# 通过索引写入指定元素\n",
    "x[0,0,2] = 9\n",
    "x"
   ],
   "id": "6fbfad56c652246d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 9.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:24:36.177748Z",
     "start_time": "2025-02-20T08:24:36.173129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pycharm对于本地文件的反应有点慢，具体文件生成效果用电脑自带的文件浏览器查看\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(\"当前工作目录是：\", current_directory)\n",
    "\n",
    "# 创建一个名为\"data\"的目录，如果目录已存在，则不会抛出错误。\n",
    "os.makedirs(os.path.join('data'), exist_ok=True)\n",
    "\n",
    "# 定义CSV文件的路径\n",
    "data_file = os.path.join('data', 'house_tiny2.csv')\n",
    "\n",
    "# 使用with语句打开文件，确保文件最后可以正确关闭\n",
    "with open(data_file, 'w') as f:\n",
    "    # 写入表头\n",
    "    f.write('NumRooms,Alley,Price\\n')\n",
    "    # 写入第一行数据\n",
    "    f.write('NA,Pave,127500\\n')\n",
    "    # 写入第二行数据\n",
    "    f.write('2,NA,106000\\n')\n",
    "    # 写入第三行数据\n",
    "    f.write('4,NA,178100\\n')\n",
    "    # 写入第四行数据\n",
    "    f.write('NA,NA,150000\\n')\n"
   ],
   "id": "78e1fd39ef556d8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录是： F:\\000-CS\\pycharm\\deep_learning_coding\\deep_learning_coding\\data\n",
      "os.path = <module 'ntpath' from 'C:\\\\Users\\\\chill\\\\.conda\\\\envs\\\\deep_learning\\\\Lib\\\\ntpath.py'>\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:25:37.287500Z",
     "start_time": "2025-02-20T08:25:37.279620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data_file = os.path.join('data', 'housing.csv')\n",
    "data = pd.read_csv(data_file)\n",
    "type(data)"
   ],
   "id": "af631bd15b2b0759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:50:10.748655Z",
     "start_time": "2025-04-16T03:50:05.894504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这其中会遇到的问题包括，数值缺失的处理\n",
    "# 处理方法一：直接抛弃缺失数据的行\n",
    "# 处理方法二，对于缺失的数据做一定处理，例如：\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# 假设已有data DataFrame的定义\n",
    "# 创建一个包含浮点数的tensor\n",
    "data = torch.tensor([\n",
    "    [1.0, 2.0,1],\n",
    "    [torch.nan, 3.0,torch.nan],\n",
    "    [4.0, torch.nan,1]\n",
    "])\n",
    "# 将tensor转换为DataFrame\n",
    "df = pd.DataFrame(data.numpy(), columns=['Feature1', 'Feature2', 'Target'])\n",
    "\n",
    "\n",
    "# 使用iloc选择DataFrame中的某些部分\n",
    "# iloc 是 \"integer location\" 的缩写\n",
    "inputs, outputs = df.iloc[:, 0:2], df.iloc[:, -1]\n",
    "\n",
    "# 使用fillna填充缺失值，这里填充的是NumRooms列的均值\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "\n",
    "# 打印inputs DataFrame查看结果\n",
    "inputs, outputs\n"
   ],
   "id": "7b3aa3aa8e9a636c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Feature1  Feature2\n",
       " 0       1.0       2.0\n",
       " 1       2.5       3.0\n",
       " 2       4.0       2.5,\n",
       " 0    1.0\n",
       " 1    NaN\n",
       " 2    1.0\n",
       " Name: Target, dtype: float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:29:27.147386Z",
     "start_time": "2025-02-20T08:29:27.140571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "# 对第一个维度，即每一列开始求和\n",
    "torch.sum(x,dim=0)"
   ],
   "id": "5855b66702e0d56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.190929Z",
     "start_time": "2025-02-20T08:17:34.185618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.tensor与numpy之间的相互转换\n",
    "# tensor->numpy\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "x.numpy()\n",
    "# numpy->tensor\n",
    "torch.tensor(x.numpy())"
   ],
   "id": "7d0777a060a5f147",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:34:43.260917Z",
     "start_time": "2025-02-20T08:34:43.254997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tensor与标量的相互转换\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ],
   "id": "ff2662387707d69e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.241964Z",
     "start_time": "2025-02-20T08:17:34.234487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 矩阵的转置\n",
    "a = torch.arange(16, dtype=torch.float32).reshape((4,4))\n",
    "# 以及判断是否是对称矩阵。如果要比较，他们的形状必须一致\n",
    "a, a.T, a==a.T"
   ],
   "id": "3e73f688f66f9729",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]),\n",
       " tensor([[ 0.,  4.,  8., 12.],\n",
       "         [ 1.,  5.,  9., 13.],\n",
       "         [ 2.,  6., 10., 14.],\n",
       "         [ 3.,  7., 11., 15.]]),\n",
       " tensor([[ True, False, False, False],\n",
       "         [False,  True, False, False],\n",
       "         [False, False,  True, False],\n",
       "         [False, False, False,  True]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.296697Z",
     "start_time": "2025-02-20T08:17:34.293696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 关于对象的引用，如果需要重新分配内存，可以使用。clone()\n",
    "a = torch.arange(16, dtype=torch.float32).reshape((4,4))\n",
    "b = a.clone()\n",
    "c = a\n"
   ],
   "id": "cfeca56bf0682c38",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.353445Z",
     "start_time": "2025-02-20T08:17:34.347621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 求和，求平均\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "# keepdim可以留作广播机制\n",
    "x, x.mean(), x.sum() / x.numel(), x.mean(dim=0), x.mean(dim=0, keepdim=True) , x.sum(dim=0) / x.shape[0]"
   ],
   "id": "2685da8eab8805a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor(5.5000),\n",
       " tensor(5.5000),\n",
       " tensor([4., 5., 6., 7.]),\n",
       " tensor([[4., 5., 6., 7.]]),\n",
       " tensor([4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.391894Z",
     "start_time": "2025-02-20T08:17:34.382606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 点积是按元素乘积求和\n",
    "x = torch.arange(12, dtype=torch.float32)\n",
    "y = torch.ones(12)\n",
    "x, y, x.dot(y)"
   ],
   "id": "acec5cfef1bb33b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor(66.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T09:08:19.575073Z",
     "start_time": "2025-02-20T09:08:19.568173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape((5,4))\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "B = torch.ones(12, dtype=torch.float32).reshape((4,3))\n",
    "# matrix-vector, matrix-matrix\n",
    "A.mv(x), A.mm(B), A@x,  A @ B"
   ],
   "id": "fe788b2de4c3a0c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 14.,  38.,  62.,  86., 110.]),\n",
       " tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]),\n",
       " tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.431330Z",
     "start_time": "2025-02-20T08:17:34.425659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# L1范数, Frobenius norm(类似L2 NORM, 平方和的开根号)\n",
    "A.abs().sum(), torch.norm(A)"
   ],
   "id": "dbef939ee053aa47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(190.), tensor(49.6991))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-16T04:07:56.127247Z",
     "start_time": "2025-04-16T04:07:55.959165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import environ\n",
    "\n",
    "import torch\n",
    "from pandas.conftest import axis_1\n",
    "\n",
    "# bug fix records:\n",
    "# error: no module named torch\n",
    "# first check the conda env in terminal `conda env list`\n",
    "# then run `python -c \"import torch; print(torch.__version__)\"` to check if torch installed\n",
    "# finally go pycharm setting to set the python interpreter to the corresponding env\n",
    "# and invalidate cache.\n",
    "\n",
    "x = torch.arange(12,30)\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n",
    "# number of element\n",
    "print(f\"{x.numel() = }\")\n",
    "# 改变形状，但不改变元素值\n",
    "print(f\"{x.reshape(3,6) = }\")\n",
    "# 通过索引访问元素\n",
    "print(f\"{x[3] = }\")\n",
    "\n",
    "# 初始化，生成一个指定的形状，内容全为0或者1\n",
    "print(f\"{torch.zeros(2,3,4) = }\")\n",
    "    # dim=0 的大小是 2，意味着在最外层维度上有2个元素。\n",
    "    # dim=1 的大小是 3，表示第二层维度上每个元素包含3个元素。\n",
    "    # dim=2 的大小是 4，表示第三层维度上每个元素包含4个元素。\n",
    "    # dim=3 的大小是 5，表示第四层维度上每个元素包含5个元素。\n",
    "print(f\"{torch.ones(2,3,4,5) = }\")\n",
    "# 初始化，创建指定值的张量\n",
    "print(f\"{torch.tensor([[1,2,3,4],[5,6,7,8]]).shape = }\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hypothesis'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m environ\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconftest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m axis_1\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# bug fix records:\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# error: no module named torch\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# first check the conda env in terminal `conda env list`\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# then run `python -c \"import torch; print(torch.__version__)\"` to check if torch installed\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# finally go pycharm setting to set the python interpreter to the corresponding env\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# and invalidate cache.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m12\u001B[39m,\u001B[38;5;241m30\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\site-packages\\pandas\\conftest.py:42\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     34\u001B[0m     TYPE_CHECKING,\n\u001B[0;32m     35\u001B[0m     Callable,\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdateutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtz\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     tzlocal,\n\u001B[0;32m     40\u001B[0m     tzutc,\n\u001B[0;32m     41\u001B[0m )\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhypothesis\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhypothesis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m strategies \u001B[38;5;28;01mas\u001B[39;00m st\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'hypothesis'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978bcecf89a65bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.541679200Z",
     "start_time": "2025-02-12T13:01:35.824390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea49450a751a735",
   "metadata": {},
   "source": [
    "![Show Element](./asset/show_element.png \"This is an example image\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b15b7862cb5793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.545784Z",
     "start_time": "2025-02-12T13:39:23.253593Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# 检查 CUDA 是否可用，并输出当前 CUDA 设备\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Current device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Current device: 0\n",
      "CUDA Device name: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c7f08513a6d995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.545784Z",
     "start_time": "2025-02-12T13:01:26.326231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edaffe1ad6b2e337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.551848200Z",
     "start_time": "2024-11-26T03:41:16.951994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "x.reshape(6,3).T.shape = torch.Size([3, 6])\n",
      "x.reshape(6,3).T = tensor([[12, 15, 18, 21, 24, 27],\n",
      "        [13, 16, 19, 22, 25, 28],\n",
      "        [14, 17, 20, 23, 26, 29]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的长度\n",
    "print(f\"{x = }\")\n",
    "\n",
    "# 张量的形状\n",
    "print(f\"{x.reshape(6,3).T.shape = }\")\n",
    "# 矩阵的转置\n",
    "print(f\"{x.reshape(6,3).T = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cb51c7138de78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "331826ad7adcbba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.552847700Z",
     "start_time": "2024-11-24T09:34:39.006521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 1., 7.],\n",
       "         [3., 7., 1.]]),\n",
       " tensor([ 6., 10., 11.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],\n",
    "                  [2,1,7],\n",
    "                  [3,7,1]], dtype=torch.float)\n",
    "identity_matrix = torch.eye(3)\n",
    "# 如果一个矩阵是对称矩阵的话，可以做如下判断，所有的值都将是True\n",
    "# *是将每个对应元素用数学乘法\n",
    "# @是矩阵乘法\n",
    "# torch.mv() 中的第i个元素是a的第i行和后面向量的点积\n",
    "a == a.T, a * identity_matrix, a @ identity_matrix, torch.mv(a,torch.tensor([1,1,1],dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab118ee62813cd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.554846500Z",
     "start_time": "2024-11-24T09:27:11.129139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 6.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求均值，求和\n",
    "torch.tensor([[1,2,3,4],\n",
    "              [5,6,7,8]],dtype=float).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5920efb7fb4a9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.556890900Z",
     "start_time": "2024-11-24T09:37:26.978358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4772)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2范数\n",
    "torch.norm(torch.tensor([1,2,3,4],dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cf5f49e4f0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1范数：\n",
    "torch.tensor([1,2,3,4],dtype=torch.float).abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24feceea26ac25",
   "metadata": {},
   "source": [
    "<img src=\"asset\\L1.png\" alt=\"L1\" width=\"400\" height=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b85402706381033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.559399800Z",
     "start_time": "2024-11-24T09:45:52.536286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵的L2范数， 它相当于是把矩阵拉成一个一维的向量，然后求范数\n",
    "torch.norm(torch.ones(4,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3a19088322a785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.560400400Z",
     "start_time": "2024-11-24T09:57:10.082224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]),\n",
       " torch.Size([4]),\n",
       " tensor([[10],\n",
       "         [26]]),\n",
       " torch.Size([4, 6, 7]),\n",
       " torch.Size([4, 9, 1, 7]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "b = torch.ones(4,9,6,7)\n",
    "# 对哪个轴求和，哪个轴的维度就会消掉; 但是如果keepdim, 那么对应的轴维度就是1\n",
    "a.shape, a.sum(axis=0).shape, a.sum(axis=1, keepdim=True), b.sum(axis = 1).shape, b.sum(axis = 2, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455ebfeb2cc619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.561401200Z",
     "start_time": "2024-11-24T09:26:24.097900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5000],\n",
       "        [6.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这样做可以保持维度，广播机制要求唯独一样。广播机制的核心是：当两个张量的形状不同时，PyTorch 会尝试自动扩展（广播）较小的张量，以匹配较大的张量形状，从而使得张量可以进行逐元素操作。\n",
    "torch.tensor([[1,2,3,4],\n",
    "              [5,6,7,8]],dtype=float).mean(axis=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3ca60c87440d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.562400300Z",
     "start_time": "2024-11-24T05:44:19.431111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4., 5., 6., 7.]),\n",
       " tensor([-1.,  0.,  1.,  2.,  3.]),\n",
       " tensor([ 2.,  4.,  6.,  8., 10.]),\n",
       " tensor([0.5000, 1.0000, 1.5000, 2.0000, 2.5000]),\n",
       " tensor([ 1.,  4.,  9., 16., 25.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2,3,4,5])\n",
    "y = torch.tensor([2,2,2,2,2])\n",
    "x+y, x-y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38a29fba21087c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.563400100Z",
     "start_time": "2024-11-24T05:56:47.646451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.],\n",
       "         [ 9., 10., 11., 12.],\n",
       "         [13., 14., 15., 16.]]),\n",
       " tensor([[ 1.,  2.,  3.,  4.,  9., 10., 11., 12.],\n",
       "         [ 5.,  6.,  7.,  8., 13., 14., 15., 16.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "Y = torch.arange(9,17,dtype=torch.float32).reshape(2,4)\n",
    "# concatenate\n",
    "torch.cat(tensors=(X,Y),dim=0), torch.cat(tensors=(X,Y),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e213c2e1805d39ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.566535900Z",
     "start_time": "2024-11-24T06:53:40.077387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False,  True, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([44, 26]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "X[0,2] = 11\n",
    "X[0,:] = 11\n",
    "Y = torch.arange(9,17,dtype=torch.float32).reshape(2,4)\n",
    "# dim 0 是列， dim 1 是行\n",
    "Y==X, X.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46cb85b683ff0e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.568042700Z",
     "start_time": "2024-11-24T06:59:51.270621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before = 2115769103248\n",
      "(before == id(Y))= True\n"
     ]
    }
   ],
   "source": [
    "before = id(Y)\n",
    "print(f\"{before = }\")\n",
    "# 会创建一个新的拷贝\n",
    "# Y = X+Y\n",
    "\n",
    "# 会原地处理\n",
    "Y[:] = X+Y\n",
    "Y += X\n",
    "print(f\"{(before == id(Y))= }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "225b3a365e99d3da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.569045500Z",
     "start_time": "2024-11-25T03:04:46.843922Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 和numpy之间的转换\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m A \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m      3\u001B[0m B \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(A)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mtype\u001B[39m(A) , \u001B[38;5;28mtype\u001B[39m(B)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# 和numpy之间的转换\n",
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A) , type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bde892e9f22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 广播机制：broadcasting mechanism \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fcfdc14f90a4c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.570045600Z",
     "start_time": "2024-11-24T07:03:18.754577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.5000), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和标量的转换\n",
    "a = torch.tensor(3.5)\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf30224d1390662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求导：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df54c80c809fb96",
   "metadata": {},
   "source": [
    "<img src=\"asset\\matrix_calculus.png\" alt=\"L1\" width=\"600\" height=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc621c48dbcadaed",
   "metadata": {},
   "source": [
    "<img src=\"asset\\matrix_calculus_shape.png\" alt=\"L1\" width=\"600\" height=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c0039d629c349",
   "metadata": {},
   "source": [
    "## 计算图：\n",
    "<img src=\"asset\\computing_graph.png\" alt=\"L1\" width=\"300\" height=\"300\" align=\"left\">\n",
    "<img src=\"asset\\back_propagation.png\" alt=\"L1\" width=\"300\" height=\"300\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec6b436cef4bc84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.571045600Z",
     "start_time": "2024-12-04T01:25:33.762716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1012., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求导：\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12, dtype=torch.float)\n",
    "# 通过调用 requires_grad_(True)，这行代码修改张量 x，使其开始追踪对其的所有操作。这是为了之后能够对其进行自动求导（自动微分）。\n",
    "# 在 PyTorch 中，函数名末尾带有下划线 _ 的函数表示它们会进行 原地操作（in-place operation），即直接修改调用它的对象而不是创建一个新的对象。\n",
    "x.requires_grad_(True)\n",
    "# 此时，x.grad 用来查看 x 的梯度。由于我们还没有进行任何计算导致梯度变化（如反向传播），x.grad 应该是 None。x.grad 存储了 x 的梯度，梯度是通过调用 .backward() 方法（执行反向传播时）计算得到的。\n",
    "x.grad\n",
    "\n",
    "y = 2 * torch.dot(x,x)\n",
    "y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894168c924a7dbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.572117300Z",
     "start_time": "2024-11-26T05:01:42.611159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36., 40., 44.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6c7e81bb8082fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.573116900Z",
     "start_time": "2024-11-26T05:01:45.059695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b4c435248dcffd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.573116900Z",
     "start_time": "2024-11-26T05:16:23.183321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下pytorch会累计梯度，我们先给它清除掉\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c546f0fb77a67cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.574125Z",
     "start_time": "2024-11-26T05:20:07.050184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y是非标量的情况(y是向量)：\n",
    "x.grad.zero_()\n",
    "y = x*x\n",
    "\n",
    "# 先对结果y做一个求和，使得结果y是一个标量\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5af30de1dc4826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:17:34.575125400Z",
     "start_time": "2024-11-26T05:23:33.641167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detach: 将计算挪到计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# detach之后，u相当于是一个常数了，而不是包含x的关系\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15312f00c62b02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 即使计算图需要通过Python的控制流，如循环，条件或者任意函数调用等，我们仍然可以计算得到变量的梯度\n",
    "\n",
    "# 这就是隐式自动求导比显示求导更方便的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e7967e7425fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be695e49d2782f6",
   "metadata": {},
   "source": [
    "配置MSVC到环境变量：\n",
    "`set PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.42.34433\\bin\\Hostx86\\x86;%PATH%`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
