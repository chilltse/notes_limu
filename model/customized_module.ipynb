{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型构造",
   "id": "f28e7c4086b48f7a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T07:13:46.816169Z",
     "start_time": "2025-04-18T07:13:45.491833Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0876, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)  # 固定的随机权重，不参与梯度更新\n",
    "        self.linear = nn.Linear(20, 20)  # 一个可训练的线性层\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)  # 第一个线性层变换\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)  # 固定权重矩阵 + 偏移 + 激活\n",
    "        X = self.linear(X)  # 再次通过线性层\n",
    "        while X.abs().sum() > 1:  # 如果总值绝对值太大，反复除以2\n",
    "            X /= 2\n",
    "        return X.sum()  # 返回最终的标量输出\n",
    "net = FixedHiddenMLP()\n",
    "print(net(torch.randn(20, 20)))"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:18:31.744755Z",
     "start_time": "2025-04-18T07:18:31.734243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 嵌套的 MLP 模块\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "# 假设 FixedHiddenMLP 已在之前定义过\n",
    "\n",
    "# 像搭积木一样组合多个模块\n",
    "chimera = nn.Sequential(\n",
    "    NestMLP(),              # 输入为 shape=(batch, 20)\n",
    "    nn.Linear(16, 20),      # 将中间结果从 16 → 20\n",
    "    FixedHiddenMLP()        # 接入前面定义的自定义模块\n",
    ")\n",
    "\n",
    "# 使用示例（假设输入为 batch size 为 2，维度为 20 的张量）\n",
    "X = torch.rand(2, 20)\n",
    "output = chimera(X)\n",
    "print(output)\n"
   ],
   "id": "c68125c6e3ecee59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4424, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 参数管理",
   "id": "6f67e2c2e80b0873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:34:32.575968Z",
     "start_time": "2025-04-18T07:34:32.565213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个简单的前馈神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),   # 输入层：4维输入 → 8维输出\n",
    "    nn.ReLU(),         # 激活函数\n",
    "    nn.Linear(8, 1)    # 输出层：8维 → 1维\n",
    ")\n",
    "\n",
    "# 输入一个 shape=(2, 4) 的批次数据（batch_size=2，每个样本4个特征）\n",
    "X = torch.rand(size=(2, 4))\n",
    "\n",
    "# 前向传播\n",
    "output = net(X)\n",
    "print(output)\n",
    "\n",
    "# 是在打印 net 中第 2 个模块的参数（权重和偏置）。\n",
    "print(net[2].state_dict())\n",
    "\n",
    "print(type(net[2].bias))  # <class 'torch.nn.parameter.Parameter'>\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.grad)\n",
    "\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "\n",
    "# 只有0和2，1拿不出来因为1是relu\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "\n",
    "\n",
    "# 可以直接访问\n",
    "print(net.state_dict()['2.bias'].data)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9d4f35aae2faa46f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1410],\n",
      "        [ 0.0042]], grad_fn=<AddmmBackward0>)\n",
      "OrderedDict({'weight': tensor([[ 0.2530,  0.1816, -0.3351,  0.2569, -0.2581, -0.0274,  0.2024,  0.3315]]), 'bias': tensor([-0.2355])})\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.2355], requires_grad=True)\n",
      "tensor([-0.2355])\n",
      "None\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "tensor([-0.2355])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:37:50.919568Z",
     "start_time": "2025-04-18T07:37:50.912560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型嵌套\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个基本模块 block1：4→8→ReLU→8→4→ReLU\n",
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 4),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "# 定义一个复合模块 block2：堆叠4个 block1\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "# 构建完整模型：block2 + Linear(4,1)\n",
    "rgnet = nn.Sequential(\n",
    "    block2(),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "# 测试输入\n",
    "X = torch.rand(2, 4)\n",
    "output = rgnet(X)\n",
    "print(output)\n",
    "\n",
    "print(rgnet)\n"
   ],
   "id": "ec650d060a19821e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4715],\n",
      "        [0.4715]], grad_fn=<AddmmBackward0>)\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:03:42.125556Z",
     "start_time": "2025-04-18T08:03:42.117196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 参数绑定\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "shared = nn.Linear(8, 8)  # 创建一个共享的线性层\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),  # net[0]\n",
    "    nn.ReLU(),        # net[1]\n",
    "    shared,           # net[2]\n",
    "    nn.ReLU(),        # net[3]\n",
    "    shared,           # net[4]\n",
    "    nn.ReLU(),        # net[5]\n",
    "    nn.Linear(8, 1)   # net[6]\n",
    ")\n",
    "\n",
    "# 输入数据\n",
    "X = torch.rand(2, 4)\n",
    "net(X)\n",
    "\n",
    "# 验证 net[2] 和 net[4] 是否共享参数（它们都引用 shared）\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])  # True\n",
    "\n",
    "# 修改 net[2] 的某个权重\n",
    "net[2].weight.data[0, 0] = 100\n",
    "\n",
    "# 再次比较两个共享层的参数\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])  # True，说明共享成功\n"
   ],
   "id": "4e3c88c2e21e37d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:10:08.235803Z",
     "start_time": "2025-04-18T08:10:08.224510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义层\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# 自定义“中心化层”：输出 = 输入 - 均值\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "# 实例化层\n",
    "layer = CenteredLayer()\n",
    "\n",
    "# 测试输入\n",
    "output = layer(torch.FloatTensor([1, 2, 3, 4, 5]))\n",
    "print(output)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(5, 8),\n",
    "    CenteredLayer(),\n",
    ")\n",
    "\n",
    "Y = net(torch.FloatTensor([1, 2, 3, 4, 5]))\n",
    "Y.mean()\n",
    "\n",
    "# 自定义线性层\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))  # 可学习的权重参数\n",
    "        self.bias = nn.Parameter(torch.randn(units))              # 可学习的偏置参数\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)  # 加上 ReLU 激活函数\n",
    "\n",
    "dense = MyLinear(5, 3)\n",
    "dense.weight"
   ],
   "id": "9e47d7609321e908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8933, -0.4900,  0.8610],\n",
       "        [ 1.5505, -0.9520,  0.9694],\n",
       "        [ 0.0120,  0.1071, -0.8248],\n",
       "        [-0.2009,  0.3434, -0.2102],\n",
       "        [-0.6891,  1.3203, -0.7312]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
