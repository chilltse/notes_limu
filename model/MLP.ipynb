{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T11:48:26.404805Z",
     "start_time": "2025-04-17T11:48:26.374066Z"
    }
   },
   "source": [
    "import ml_model_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:48:52.396172Z",
     "start_time": "2025-04-17T11:48:26.416120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# print(os.getcwd())\n",
    "# 坑：直接命名utils会产生冲突，所以改成ml_utils\n",
    "from ml_model_utils import train_ch3\n",
    "num_input, num_output, num_hidden = 784, 10, 256\n",
    "\n",
    "\n",
    "# TODO 测试全部参数初始成0会怎样\n",
    "# 测试结果，梯度更新失败：对称性未打破 → 所有神经元收到一样的“信号” → 学习陷入同步 → 没有分工 → 学不到复杂的函数！\n",
    "W1 = nn.Parameter(torch.randn(num_input, num_hidden, requires_grad=True))\n",
    "b1 = nn.Parameter(torch.zeros(num_hidden, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(num_hidden, num_output, requires_grad=True))\n",
    "b2 = nn.Parameter(torch.zeros(num_output, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "def relu(X):\n",
    "    # 生成一个和 X 形状一样、元素全为 0 的张量。\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)\n",
    "\n",
    "def net(X):\n",
    "    # 两个括号也可以写成一个括号\n",
    "    X = X.reshape((-1, num_input))\n",
    "    # ↓输出的形状 batch_size * 1\n",
    "    H = relu(X @ W1 + b1)\n",
    "    # 正好我们设置的hidden_layer数量和batch_size一致，所以能乘\n",
    "    return H @ W2 + b2\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
    "\n",
    "\n"
   ],
   "id": "37f06df670a7bd38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_metrics = (12.194949308141073, 0.7001666666666667)\n",
      "test_acc = 0.7346\n",
      "train_metrics = (2.1400807099024455, 0.74385)\n",
      "test_acc = 0.7422\n",
      "train_metrics = (1.5038133860905964, 0.7557833333333334)\n",
      "test_acc = 0.7511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 34\u001B[0m\n\u001B[0;32m     32\u001B[0m num_epochs, lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m0.1\u001B[39m\n\u001B[0;32m     33\u001B[0m updater \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(params, lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m---> 34\u001B[0m train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "File \u001B[1;32mF:\\000-CS\\pycharm\\deep_learning_coding\\deep_learning_coding\\LIMU_BILIBILI\\model\\ml_utils.py:110\u001B[0m, in \u001B[0;36mtrain_ch3\u001B[1;34m(net, train_iter, test_iter, loss, num_epochs, updater)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_ch3\u001B[39m(net, train_iter, test_iter, loss, num_epochs, updater):\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m    109\u001B[0m         \u001B[38;5;66;03m# 训练一轮，返回平均 loss 和准确率\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m         train_metrics \u001B[38;5;241m=\u001B[39m train_epoch_ch3(net, train_iter, loss, updater)\n\u001B[0;32m    112\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_metrics\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m= }\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;66;03m# 在测试集上评估准确率\u001B[39;00m\n",
      "File \u001B[1;32mF:\\000-CS\\pycharm\\deep_learning_coding\\deep_learning_coding\\LIMU_BILIBILI\\model\\ml_utils.py:79\u001B[0m, in \u001B[0;36mtrain_epoch_ch3\u001B[1;34m(net, train_iter, loss, updater)\u001B[0m\n\u001B[0;32m     76\u001B[0m     net\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# 设置为训练模式\u001B[39;00m\n\u001B[0;32m     77\u001B[0m metric \u001B[38;5;241m=\u001B[39m Accumulator(\u001B[38;5;241m3\u001B[39m)  \u001B[38;5;66;03m# [累加loss, 预测正确数, 样本总数]\u001B[39;00m\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X, y \u001B[38;5;129;01min\u001B[39;00m train_iter:\n\u001B[0;32m     80\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m net(X)              \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[0;32m     81\u001B[0m     l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)          \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    714\u001B[0m ):\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1455\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1457\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1458\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_data()\n\u001B[0;32m   1459\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1461\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1420\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1416\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1417\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1419\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1420\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_get_data()\n\u001B[0;32m   1421\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1422\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m   1239\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m   1240\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1248\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m   1249\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m   1250\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1251\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_queue\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m   1252\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1254\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m   1255\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m   1256\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\multiprocessing\\queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[0;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\multiprocessing\\connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout)\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\multiprocessing\\connection.py:346\u001B[0m, in \u001B[0;36mPipeConnection._poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m    344\u001B[0m             _winapi\u001B[38;5;241m.\u001B[39mPeekNamedPipe(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(wait([\u001B[38;5;28mself\u001B[39m], timeout))\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\multiprocessing\\connection.py:1084\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_list, timeout)\u001B[0m\n\u001B[0;32m   1081\u001B[0m                 ready_objects\u001B[38;5;241m.\u001B[39madd(o)\n\u001B[0;32m   1082\u001B[0m                 timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1084\u001B[0m     ready_handles \u001B[38;5;241m=\u001B[39m _exhaustive_wait(waithandle_to_obj\u001B[38;5;241m.\u001B[39mkeys(), timeout)\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[0;32m   1087\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "File \u001B[1;32m~\\.conda\\envs\\deep_learning\\Lib\\multiprocessing\\connection.py:1016\u001B[0m, in \u001B[0;36m_exhaustive_wait\u001B[1;34m(handles, timeout)\u001B[0m\n\u001B[0;32m   1014\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[1;32m-> 1016\u001B[0m     res \u001B[38;5;241m=\u001B[39m _winapi\u001B[38;5;241m.\u001B[39mWaitForMultipleObjects(L, \u001B[38;5;28;01mFalse\u001B[39;00m, timeout)\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m WAIT_TIMEOUT:\n\u001B[0;32m   1018\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:50:58.746071Z",
     "start_time": "2025-04-17T11:49:43.780434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MLP的简洁实现\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),                          # 将输入展平为 (batch_size, 784)\n",
    "    nn.Linear(784, 256),                   # 第一层：输入 784 → 隐藏层 256\n",
    "    nn.ReLU(),                             # 激活函数 ReLU\n",
    "    nn.Linear(256, 10)                     # 第二层：隐藏层 256 → 输出层 10 类\n",
    ")\n",
    "\n",
    "# 解释：m是什么\n",
    "# m 是 nn.Module 中的每一层（layer），也就是子模块，比如：nn.Linear、nn.ReLU、nn.Conv2d 等。\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# apply()它会：\n",
    "# 遍历模型中所有的 子模块（包括嵌套的）\n",
    "# 对每一层调用 fn(m)，其中 m 就是该子模块\n",
    "net.apply(init_weights)\n",
    "\n",
    "batch_size, lr = 256,  0.1\n",
    "num_epochs = 10\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n",
    "\n",
    "\n",
    "# MLP的好处是，互相转换成本不高，因为卷积，transformer等等都类似，但是svm就更麻烦\n",
    "\n",
    "\n"
   ],
   "id": "aab60557fdf5675e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_metrics = (1.0418544942220052, 0.642)\n",
      "test_acc = 0.7063\n",
      "train_metrics = (0.5958714768409729, 0.7903333333333333)\n",
      "test_acc = 0.7559\n",
      "train_metrics = (0.5159378643035889, 0.8201333333333334)\n",
      "test_acc = 0.7852\n",
      "train_metrics = (0.4769068436463674, 0.8322)\n",
      "test_acc = 0.8082\n",
      "train_metrics = (0.45114819539388024, 0.8418)\n",
      "test_acc = 0.8218\n",
      "train_metrics = (0.43160968311627707, 0.8491833333333333)\n",
      "test_acc = 0.832\n",
      "train_metrics = (0.41573027930259704, 0.8542166666666666)\n",
      "test_acc = 0.8397\n",
      "train_metrics = (0.40237933432261147, 0.8595166666666667)\n",
      "test_acc = 0.8441\n",
      "train_metrics = (0.39116849249204, 0.8633666666666666)\n",
      "test_acc = 0.8474\n",
      "train_metrics = (0.38131968547503153, 0.8662833333333333)\n",
      "test_acc = 0.8506\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
