{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:08:34.507854Z",
     "start_time": "2025-02-12T08:08:34.319430Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environ\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconftest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m axis_1\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# bug fix records:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# error: no module named torch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# first check the conda env in terminal `conda env list` \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# then run `python -c \"import torch; print(torch.__version__)\"` to check if torch installed \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# finally go pycharm setting to set the python interpreter to the corresponding env \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# and invalidate cache.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "import torch\n",
    "from pandas.conftest import axis_1\n",
    "\n",
    "# bug fix records:\n",
    "# error: no module named torch\n",
    "# first check the conda env in terminal `conda env list` \n",
    "# then run `python -c \"import torch; print(torch.__version__)\"` to check if torch installed \n",
    "# finally go pycharm setting to set the python interpreter to the corresponding env \n",
    "# and invalidate cache.\n",
    "\n",
    "x = torch.arange(12,30)\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n",
    "# number of element\n",
    "print(f\"{x.numel() = }\")\n",
    "# 改变形状，但不改变元素值\n",
    "print(f\"{x.reshape(3,6) = }\")\n",
    "# 通过索引访问元素\n",
    "print(f\"{x[3] = }\")\n",
    "\n",
    "# 初始化，生成一个指定的形状，内容全为0或者1\n",
    "print(f\"{torch.zeros(2,3,4) = }\")\n",
    "    # dim=0 的大小是 2，意味着在最外层维度上有2个元素。\n",
    "    # dim=1 的大小是 3，表示第二层维度上每个元素包含3个元素。\n",
    "    # dim=2 的大小是 4，表示第三层维度上每个元素包含4个元素。\n",
    "    # dim=3 的大小是 5，表示第四层维度上每个元素包含5个元素。\n",
    "print(f\"{torch.ones(2,3,4,5) = }\")\n",
    "# 初始化，创建指定值的张量\n",
    "print(f\"{torch.tensor([[1,2,3,4],[5,6,7,8]]).shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978bcecf89a65bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:01:35.827960Z",
     "start_time": "2025-02-12T13:01:35.824390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea49450a751a735",
   "metadata": {},
   "source": [
    "![Show Element](./asset/show_element.png \"This is an example image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b15b7862cb5793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:01:23.138584Z",
     "start_time": "2025-02-12T13:01:21.081029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Current device: 0\n",
      "CUDA Device name: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 CUDA 是否可用，并输出当前 CUDA 设备\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Current device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c7f08513a6d995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:01:26.329902Z",
     "start_time": "2025-02-12T13:01:26.326231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edaffe1ad6b2e337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T03:41:16.983790Z",
     "start_time": "2024-11-26T03:41:16.951994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "x.reshape(6,3).T.shape = torch.Size([3, 6])\n",
      "x.reshape(6,3).T = tensor([[12, 15, 18, 21, 24, 27],\n",
      "        [13, 16, 19, 22, 25, 28],\n",
      "        [14, 17, 20, 23, 26, 29]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的长度\n",
    "print(f\"{x = }\")\n",
    "\n",
    "# 张量的形状\n",
    "print(f\"{x.reshape(6,3).T.shape = }\")\n",
    "# 矩阵的转置\n",
    "print(f\"{x.reshape(6,3).T = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cb51c7138de78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "331826ad7adcbba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:34:39.023045Z",
     "start_time": "2024-11-24T09:34:39.006521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 1., 7.],\n",
       "         [3., 7., 1.]]),\n",
       " tensor([ 6., 10., 11.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],\n",
    "                  [2,1,7],\n",
    "                  [3,7,1]], dtype=torch.float)\n",
    "identity_matrix = torch.eye(3)\n",
    "# 如果一个矩阵是对称矩阵的话，可以做如下判断，所有的值都将是True\n",
    "# *是将每个对应元素用数学乘法\n",
    "# @是矩阵乘法\n",
    "# torch.mv() 中的第i个元素是a的第i行和后面向量的点积\n",
    "a == a.T, a * identity_matrix, a @ identity_matrix, torch.mv(a,torch.tensor([1,1,1],dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab118ee62813cd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:27:11.138448Z",
     "start_time": "2024-11-24T09:27:11.129139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 6.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求均值，求和\n",
    "torch.tensor([[1,2,3,4],\n",
    "              [5,6,7,8]],dtype=float).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5920efb7fb4a9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:37:27.001661Z",
     "start_time": "2024-11-24T09:37:26.978358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4772)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2范数\n",
    "torch.norm(torch.tensor([1,2,3,4],dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cf5f49e4f0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1范数：\n",
    "torch.tensor([1,2,3,4],dtype=torch.float).abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24feceea26ac25",
   "metadata": {},
   "source": [
    "<img src=\"asset\\L1.png\" alt=\"L1\" width=\"400\" height=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b85402706381033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:45:52.554009Z",
     "start_time": "2024-11-24T09:45:52.536286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵的L2范数， 它相当于是把矩阵拉成一个一维的向量，然后求范数\n",
    "torch.norm(torch.ones(4,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3a19088322a785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:57:10.096622Z",
     "start_time": "2024-11-24T09:57:10.082224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]),\n",
       " torch.Size([4]),\n",
       " tensor([[10],\n",
       "         [26]]),\n",
       " torch.Size([4, 6, 7]),\n",
       " torch.Size([4, 9, 1, 7]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "b = torch.ones(4,9,6,7)\n",
    "# 对哪个轴求和，哪个轴的维度就会消掉; 但是如果keepdim, 那么对应的轴维度就是1\n",
    "a.shape, a.sum(axis=0).shape, a.sum(axis=1, keepdim=True), b.sum(axis = 1).shape, b.sum(axis = 2, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455ebfeb2cc619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:26:24.106146Z",
     "start_time": "2024-11-24T09:26:24.097900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5000],\n",
       "        [6.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这样做可以保持维度，广播机制要求唯独一样。广播机制的核心是：当两个张量的形状不同时，PyTorch 会尝试自动扩展（广播）较小的张量，以匹配较大的张量形状，从而使得张量可以进行逐元素操作。\n",
    "torch.tensor([[1,2,3,4],\n",
    "              [5,6,7,8]],dtype=float).mean(axis=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3ca60c87440d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:44:19.495980Z",
     "start_time": "2024-11-24T05:44:19.431111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4., 5., 6., 7.]),\n",
       " tensor([-1.,  0.,  1.,  2.,  3.]),\n",
       " tensor([ 2.,  4.,  6.,  8., 10.]),\n",
       " tensor([0.5000, 1.0000, 1.5000, 2.0000, 2.5000]),\n",
       " tensor([ 1.,  4.,  9., 16., 25.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2,3,4,5])\n",
    "y = torch.tensor([2,2,2,2,2])\n",
    "x+y, x-y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38a29fba21087c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:56:47.656247Z",
     "start_time": "2024-11-24T05:56:47.646451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.],\n",
       "         [ 9., 10., 11., 12.],\n",
       "         [13., 14., 15., 16.]]),\n",
       " tensor([[ 1.,  2.,  3.,  4.,  9., 10., 11., 12.],\n",
       "         [ 5.,  6.,  7.,  8., 13., 14., 15., 16.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "Y = torch.arange(9,17,dtype=torch.float32).reshape(2,4)\n",
    "# concatenate\n",
    "torch.cat(tensors=(X,Y),dim=0), torch.cat(tensors=(X,Y),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e213c2e1805d39ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:53:40.097550Z",
     "start_time": "2024-11-24T06:53:40.077387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False,  True, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([44, 26]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "X[0,2] = 11\n",
    "X[0,:] = 11\n",
    "Y = torch.arange(9,17,dtype=torch.float32).reshape(2,4)\n",
    "# dim 0 是列， dim 1 是行\n",
    "Y==X, X.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46cb85b683ff0e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:59:51.304383Z",
     "start_time": "2024-11-24T06:59:51.270621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before = 2115769103248\n",
      "(before == id(Y))= True\n"
     ]
    }
   ],
   "source": [
    "before = id(Y)\n",
    "print(f\"{before = }\")\n",
    "# 会创建一个新的拷贝\n",
    "# Y = X+Y\n",
    "\n",
    "# 会原地处理\n",
    "Y[:] = X+Y\n",
    "Y += X\n",
    "print(f\"{(before == id(Y))= }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "225b3a365e99d3da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:04:46.993724Z",
     "start_time": "2024-11-25T03:04:46.843922Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 和numpy之间的转换\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m A \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(A)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mtype\u001b[39m(A) , \u001b[38;5;28mtype\u001b[39m(B)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# 和numpy之间的转换\n",
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A) , type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bde892e9f22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 广播机制：broadcasting mechanism \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fcfdc14f90a4c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:03:18.773649Z",
     "start_time": "2024-11-24T07:03:18.754577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.5000), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和标量的转换\n",
    "a = torch.tensor(3.5)\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf30224d1390662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求导：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df54c80c809fb96",
   "metadata": {},
   "source": [
    "<img src=\"asset\\matrix_calculus.png\" alt=\"L1\" width=\"600\" height=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc621c48dbcadaed",
   "metadata": {},
   "source": [
    "<img src=\"asset\\matrix_calculus_shape.png\" alt=\"L1\" width=\"600\" height=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c0039d629c349",
   "metadata": {},
   "source": [
    "## 计算图：\n",
    "<img src=\"asset\\computing_graph.png\" alt=\"L1\" width=\"300\" height=\"300\" align=\"left\">\n",
    "<img src=\"asset\\back_propagation.png\" alt=\"L1\" width=\"300\" height=\"300\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec6b436cef4bc84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T01:25:33.809867Z",
     "start_time": "2024-12-04T01:25:33.762716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1012., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求导：\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12, dtype=torch.float)\n",
    "# 通过调用 requires_grad_(True)，这行代码修改张量 x，使其开始追踪对其的所有操作。这是为了之后能够对其进行自动求导（自动微分）。\n",
    "# 在 PyTorch 中，函数名末尾带有下划线 _ 的函数表示它们会进行 原地操作（in-place operation），即直接修改调用它的对象而不是创建一个新的对象。\n",
    "x.requires_grad_(True)\n",
    "# 此时，x.grad 用来查看 x 的梯度。由于我们还没有进行任何计算导致梯度变化（如反向传播），x.grad 应该是 None。x.grad 存储了 x 的梯度，梯度是通过调用 .backward() 方法（执行反向传播时）计算得到的。\n",
    "x.grad\n",
    "\n",
    "y = 2 * torch.dot(x,x)\n",
    "y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894168c924a7dbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:01:42.638271Z",
     "start_time": "2024-11-26T05:01:42.611159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36., 40., 44.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6c7e81bb8082fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:01:45.075061Z",
     "start_time": "2024-11-26T05:01:45.059695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b4c435248dcffd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:16:23.199254Z",
     "start_time": "2024-11-26T05:16:23.183321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下pytorch会累计梯度，我们先给它清除掉\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c546f0fb77a67cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:20:07.081680Z",
     "start_time": "2024-11-26T05:20:07.050184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y是非标量的情况(y是向量)：\n",
    "x.grad.zero_()\n",
    "y = x*x\n",
    "\n",
    "# 先对结果y做一个求和，使得结果y是一个标量\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5af30de1dc4826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:23:33.656182Z",
     "start_time": "2024-11-26T05:23:33.641167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detach: 将计算挪到计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# detach之后，u相当于是一个常数了，而不是包含x的关系\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15312f00c62b02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 即使计算图需要通过Python的控制流，如循环，条件或者任意函数调用等，我们仍然可以计算得到变量的梯度\n",
    "\n",
    "# 这就是隐式自动求导比显示求导更方便的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e7967e7425fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be695e49d2782f6",
   "metadata": {},
   "source": [
    "配置MSVC到环境变量：\n",
    "`set PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.42.34433\\bin\\Hostx86\\x86;%PATH%`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
