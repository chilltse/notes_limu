{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T10:03:51.510017Z",
     "start_time": "2025-09-09T10:03:44.555751Z"
    }
   },
   "source": [
    "import torch\n",
    "# 导入 PyTorch 的负对数似然损失函数（Negative Log Likelihood Loss）。虽然在这段代码中没有使用，但它和信息量（self-information）以及对数概率的概念相关。\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "# 作用：对所有有效的数字进行求和\n",
    "def nansum(x):\n",
    "    # torch.isnan(x) 返回一个布尔张量，标记 x 中哪些位置是 NaN（Not a number非数字）。\n",
    "    # ~torch.isnan(x) 对布尔张量取反，即只保留是数字的位置。\n",
    "    # x[~torch.isnan(x)]：过滤掉 NaN 元素，只留下有效数值。\n",
    "    return x[~torch.isnan(x)].sum()\n",
    "\n",
    "# 自信息量等于 概率的负对数\n",
    "def self_information(p):\n",
    "    return -torch.log2(torch.tensor(p)).item()\n",
    "\n",
    "print(self_information(1 / 64))\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    entropy = - p * torch.log2(p)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = nansum(entropy)\n",
    "    return out\n",
    "\n",
    "entropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))\n",
    "\n",
    "# 这里的代码其实和entropy一模一样，只是在数学上对两个随机变量的联合概率分布有新的解释。\n",
    "def joint_entropy(p_xy):\n",
    "    joint_ent = -p_xy * torch.log2(p_xy)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = nansum(joint_ent)\n",
    "    return out\n",
    "\n",
    "# H(X,Y) = -(0.1 log2 0.1 + 0.5 log2 0.5 + 0.1 log2 0.1 + 0.3 log2 0.3)\n",
    "joint_entropy(torch.tensor([[0.1, 0.5],\n",
    "                            [0.1, 0.3]]))\n",
    "\n",
    "\n",
    "def conditional_entropy(p_xy, p_x):\n",
    "    p_y_given_x = p_xy/p_x\n",
    "    print(\"p_y_given_x\")\n",
    "    print(p_y_given_x)\n",
    "    cond_ent = -p_xy * torch.log2(p_y_given_x)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = nansum(cond_ent)\n",
    "    return out\n",
    "\n",
    "# 这里的概率分布有点奇怪\n",
    "conditional_entropy(torch.tensor([[0.1, 0.5], [0.2, 0.3]]),\n",
    "                    torch.tensor([0.2, 0.8]))\n",
    "\n",
    "\n",
    "\n",
    "def mutual_information(p_xy, p_x, p_y):\n",
    "    p = p_xy / (p_x * p_y)\n",
    "    mutual = p_xy * torch.log2(p)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = nansum(mutual)\n",
    "    return out\n",
    "\n",
    "mutual_information(torch.tensor([[0.1, 0.5], [0.1, 0.3]]),\n",
    "                   torch.tensor([0.2, 0.8]), torch.tensor([[0.75, 0.25]]))\n",
    "\n",
    "\n",
    "\n",
    "# KL散度：\n",
    "def kl_divergence(p, q):\n",
    "    kl = p * torch.log2(p / q)\n",
    "    out = nansum(kl)\n",
    "    # 这里里有问题，因为如果p,q归一化了，它自然回事大于等于零的\n",
    "    return out.abs().item()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "p_y_given_x\n",
      "tensor([[0.5000, 0.6250],\n",
      "        [1.0000, 0.3750]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "下面关于对称性的实验种用到了：\n",
    "$\\text{Relative Difference} = \\frac{|a - b|}{\\tfrac{|a| + |b|}{2}}$"
   ],
   "id": "cab786e0a9f59f55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:11:02.240695Z",
     "start_time": "2025-09-09T10:11:02.230788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# KL散度例子：\n",
    "# 生成并排序三个长度为 10,000 的张量：\n",
    "#     p：服从标准正态分布 N(0,1)。\n",
    "#     q1：服从 N(−1,1)。\n",
    "#     q2：服从 N(1,1)。\n",
    "\n",
    "# torch.manual_seed(seed) 会设置 当前 Python 进程内 的 CPU 和默认 GPU（如果有的话） 的随机数生成器（RNG）的种子。\n",
    "torch.manual_seed(1)\n",
    "\n",
    "tensor_len = 10000\n",
    "p = torch.normal(0, 1, (tensor_len, ))\n",
    "q1 = torch.normal(-1, 1, (tensor_len, ))\n",
    "q2 = torch.normal(1, 1, (tensor_len, ))\n",
    "\n",
    "p = torch.sort(p)[0]\n",
    "q1 = torch.sort(q1)[0]\n",
    "q2 = torch.sort(q2)[0]\n",
    "\n",
    "# 对称性实验\n",
    "# q1和q2关于x=0对称；预期p与q1的散度 与 p与q2的散度接近\n",
    "kl_pq1 = kl_divergence(p, q1)\n",
    "kl_pq2 = kl_divergence(p, q2)\n",
    "similar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100\n",
    "\n",
    "print(kl_pq1, kl_pq2, similar_percentage)\n",
    "# 输出: (8582.034..., 8828.309..., 2.83...)\n",
    "\n",
    "# 不对称性测试\n",
    "# 对比q2与p的散度 与 p与q2的散度：\n",
    "kl_q2p = kl_divergence(q2, p)\n",
    "differ_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100\n",
    "\n",
    "print(kl_q2p, differ_percentage)\n",
    "# 输出: (14130.125, 46.18...)\n"
   ],
   "id": "4f2495c2ada5e150",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8582.0341796875 8828.3095703125 2.8290698237936858\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
